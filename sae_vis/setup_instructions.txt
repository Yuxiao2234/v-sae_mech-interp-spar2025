# Fast SAE Visualization

A clean, fast implementation for visualizing Sparse Autoencoder (SAE) features. Designed specifically for VSAETopK and AutoEncoderTopK models, but extensible to other SAE architectures.

## Installation

Replace the existing `spar/sae_vis/` directory with these files:

```
spar/
├── sae_vis/                    # Replace the entire directory
│   ├── __init__.py            # Package initialization  
│   ├── sae_interface.py       # Unified SAE model adapter
│   ├── feature_analyzer.py    # Core analysis engine  
│   ├── html_generator.py      # HTML visualization generator
│   ├── example_usage.py       # Working examples
│   ├── quick_test.py          # Standalone test script
│   └── README.md              # This file
├── trainers/                  # Your existing trainers
└── dictionary_learning/       # Your existing modules
```

## Quick Start

### Option 1: One-Line Usage (Easiest)

```python
import sys
sys.path.append('path/to/spar')

from sae_vis import create_visualization
from trainers.top_k import AutoEncoderTopK

# Load your models
model = HookedTransformer.from_pretrained("gelu-1l")
sae_model = AutoEncoderTopK.from_pretrained("path/to/your/sae.pt")

# Create visualization in one line!
results = create_visualization(
    sae_model=sae_model,
    transformer_model=model,
    tokenizer=model.tokenizer,
    texts=["Your text corpus here..."],
    feature_indices=[0, 1, 2, 3, 4],
    output_path="sae_viz.html"
)
```

### Option 2: Step-by-Step Usage

```python
from sae_vis import UnifiedSAEInterface, FeatureAnalyzer, HTMLGenerator

# Create analyzer
sae_interface = UnifiedSAEInterface(sae_model)
analyzer = FeatureAnalyzer(sae_interface, model, model.tokenizer)

# Analyze features
results = analyzer.analyze_features(
    texts=["Your text corpus here..."],
    feature_indices=[0, 1, 2, 3, 4],
    max_examples=10
)

# Generate visualization
html_gen = HTMLGenerator()
html_gen.create_visualization(results, "sae_viz.html")
```

### Testing the Installation

```bash
# From spar/sae_vis/ directory:
python quick_test.py

# Or from spar/ directory:
python sae_vis/quick_test.py
```

The generated HTML visualization includes:

- **Feature Statistics**: Sparsity, activation statistics, decoder norms
- **Top Activating Examples**: Text contexts where each feature fires most strongly
- **Token Highlighting**: Visual indication of activation strength per token
- **Logit Effects**: Which tokens the feature promotes/suppresses
- **Interactive Browsing**: Navigate between multiple features

## Supported Models

### AutoEncoderTopK
Your standard TopK SAE implementation. The interface expects:
- `encode(x, return_topk=True)` → returns `(sparse_features, top_values, top_indices, pre_activation)`
- `decode(features)` → returns reconstruction
- Standard weight matrices accessible

### VSAETopK  
Your variational TopK SAE. The interface expects:
- `encode(x, return_topk=True, training=False)` → returns `(sparse_features, z, mu, log_var, top_indices, selected_vals)`
- `decode(features)` → returns reconstruction

### Adding New Model Types

To support a new SAE architecture, modify `UnifiedSAEInterface._detect_model_type()` and add the appropriate handling in the `encode()` method.

## Configuration Options

### FeatureAnalyzer.analyze_features()

- `texts`: List of strings to analyze
- `feature_indices`: Which features to analyze (e.g., `[0, 1, 2]`)
- `max_examples`: Top-k examples per feature (default: 20)
- `max_seq_len`: Maximum sequence length (default: 512)
- `batch_size`: Processing batch size (default: 8)

### HTMLGenerator.create_visualization()

- `feature_analyses`: Results from analyzer
- `output_path`: Where to save HTML file
- `title`: Title for the visualization

## Performance Tips

1. **Start Small**: Test with 5-10 features first
2. **Batch Size**: Increase batch_size if you have lots of GPU memory
3. **Text Corpus**: More diverse text = better feature understanding
4. **Feature Selection**: Focus on features with interesting sparsity patterns

## Troubleshooting

### "Cannot find decoder weights"
Make sure your SAE model has either:
- `sae.decoder.weight` (standard PyTorch Linear layer)
- `sae.W_dec` (custom weight matrix)

### "Encode method not found" 
Verify your SAE model has an `encode()` method that accepts `return_topk=True`.

### "Dimension mismatch"
Check that your SAE was trained on the same model/layer you're analyzing. The `activation_dim` should match the transformer's hidden dimension at that layer.

### Debug Your Model
Use the debug function:

```python
from example_usage import debug_sae_interface
debug_sae_interface()  # Will print model info and test encoding
```

## Architecture Notes

This implementation is designed to be:

1. **Fast**: Batch processing, efficient TopK usage
2. **Clean**: Minimal abstractions, clear data flow
3. **Extensible**: Easy to add new model types or visualization features
4. **Self-contained**: No complex dependencies

The core insight is that TopK SAE models already provide exactly the information we need for visualization - we just need to use it efficiently rather than fighting against it.

## Extending the System

### Adding New Visualizations
Modify `HTMLGenerator` to add new sections or interactivity.

### Different Analysis Types  
Extend `FeatureAnalyzer` to compute additional statistics or use different activation patterns.

### Alternative Output Formats
Create new generator classes for JSON, Observable notebooks, React components, etc.

## Comparison to Original sae_vis

| Original sae_vis | Fast SAE Vis |
|------------------|--------------|
| Complex config system | Simple function arguments |
| Many abstraction layers | Direct, linear flow |
| Fights tuple returns | Embraces TopK naturally |
| Slow batch processing | Efficient batching |
| Hard to extend | Clean, modular design |

This implementation focuses on the core value: **understanding what SAE features detect** through clean visualizations, without unnecessary complexity.
